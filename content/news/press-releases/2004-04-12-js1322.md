---
title: 'John B. Taylor Under Secretary of the Treasury for International Affairs At the Conference “Models and Monetary Policy: Research in the Tradition of Dale Henderson, Richard Porter, and Peter Tinsley”'
date: 2004-04-12T08:00:00-04:00
draft: false
url: /news/press-releases/js1322
---
*(Archived Content)*

FROM THE OFFICE OF PUBLIC AFFAIRS

*To view or print the PDF content on this page, download the free Adobe® Acrobat® Reader®. *

js-1322

Federal Reserve Board March 27, 2004

It is a pleasure for me to participate in this conference honoring Dale Henderson, Richard Porter, and Peter Tinsley, three trail blazers in the field economic modeling and its application to economic policy.  I thank Jon Faust, Athansios Orphanides, and David Reisfschneider for organizing the conference and for inviting me.

**Monetary Policy Modeling: Where Are We?**

Today, stochastic simulation of econometric models and dynamic optimization with such models as a way to deal with real world uncertainty is a commonplace part of policy analysis.  It is hard to imagine, now, what it was like formulating monetary policy without such models.  Yet, back in the days before Peter Tinsley was directing the Fed’s  “Special Studies” section, or before Dick Porter was directing the “Econometrics and Computer Applications” section, or before Dale Henderson was advising and leading countless young researchers in the Division of International Finance, that was what it was like.   Then it was only a dream that new quantitative methods could be applied in practice to improve monetary policy decisions.  Today it is a reality—a dream come true—thanks to the leadership of Dale, Dick, and Peter and to the many others they worked with inside and outside of the Fed.

I believe the application of these methods in the area of monetary policy has made a real positive difference. It is a fact, of course, that macroeconomic performance has improved: the variability of real output is substantially lower, as are fluctuations of inflation, and inflation itself. There is a raging debate about the reasons for this improvement, but a more dynamic, quantitative, and systematic approach to monetary policy decisions in an explicitly uncertain environment must be given substantial credit.[[1]](https://home.treasury.gov/news/press-releases/js1322#_ftn1_ftn1)Interest rate decisions by the Fed and other central banks around the world have become more systematic, explicit, and responsive. This has been instrumental in helping push the world economy from the bad old days of high inflation and output instability.

The published research record of Dale, Dick, and Peter is substantial.  It demonstrates their originality and technical firepower, but it understates the role they played in pioneering the use of quantitative models in policy analysis.  Applying economic models in policy is not easy.  It requires much more than mathematical and statistical knowledge and experience with how the models work, though these of course are prerequisites.  It requires knowledge of the policy making process, including the politics and the personalities.  It requires good judgment, for example, about when model complexity or elegance must be sacrificed to create a more practical and constructive policy framework. It requires leadership and management skills to get the best team; this includes recruiting, retaining, and motivating.

I have been fascinated and have written about this nexus between basic research and its application to policy, using the word “translational economics” which I borrowed from the work on “translational biology,” the study of the interface between biological research and its application to the medical fields. As I have said, “There are plenty of good economic ideas ‘out there on the shelf’ that do not affect practical decision making. Some direct action, perhaps by those working in the policy arena or perhaps by people close to policy making, are needed to take the academic research and to mold it into something useful to policy makers.”[[2]](https://home.treasury.gov/news/press-releases/js1322#_ftn2_ftn2)Dale, Dick, and Peter are masters of translational economics as well as basic economic research.

** And Where Should We Be Going?**

I think it would be most constructive if I approached this question by focusing on how we are using, and are planning to use, quantitative modeling to improve policy decisions in the areas of international monetary and financial policy at the U.S. Treasury. In fact, there are many common issues that arise both when applying quantitative methods to monetary policy and when applying quantitative methods to other areas of economic policy.

First, let me note that, in order to place greater emphasis on quantitative modeling—and here I include basic empirical work and the development of dynamic models dealing with uncertainty—three years ago we created a new section in International Affairs at Treasury.  It is called Quantitative Policy Analysis.  Like so many things in government it is now referred to by its acronym, QPA. It is a “cross cutting” section, interacting with the country desks and international financial institution offices of Treasury.

Second, I am grateful that we at Treasury have been able benefit from the quantitative modeling capability at the Fed.  For example, we have benefited from work on the Federal Reserve Board staff’s new computable general equilibrium model (SIGMA) to backup our discussions of the current account and supply side policies in recent meetings of Working Party Three of the OECD.  New models like this one have the potential to be the workhorses of the new “Agenda for Growth” in which structural or supply-side policies are the main focus, much as earlier models have been the workhorses of policy in the monetary field. They are examples, in my view, of where we should be going.

Let me now briefly describe three specific policy applications that have developed as part of the work of the Office of Quantitative Policy Analysis.  They are also examples of where we should be going.

** Millennium Challenge Account: Quantitative Indicators of Pro-Growth Policies ** It is a basic principle of economics that reducing poverty in poor countries requires much higher economic growth. Countries are poor because productivity is low. To reduce poverty, poor countries must have much higher productivity growth rates.  The academic, empirical growth literature (and much experience) has shown that good economic policy has much to do with increasing economic growth.  In order to raise economic growth in poor countries, therefore, President Bush proposed a new Millennium Challenge Account (MCA), which encourages pro-growth policies in poor countries by allocating more aid to countries that are actually following good pro-growth policies.

Making the MCA operational has required a quantitative approach.  In order to determine which countries would qualify for assistance, we had to find objective indicators of pro-growth policies.   The methodology evaluates three broad categories of policy performance 1) governing justly, 2) investing in people, and 3) encouraging economic freedom.  We then selected 16 quantitative indicators in these categories based on their relationship to economic growth.  We tried to keep the number of indicators small and make sure they were available for a large number of countries. We consulted with many people both inside and outside government in deciding on the indicators ** We developed a robust procedure for combining the 16 indicators: put simply, a particular candidate country must perform above the median on at least half of the indicators in each of the three policy categories.

Ultimately, a board consisting of the Secretary of State, the Secretary of the Treasury, and others will determine funding allocations for the poor countries.  They will use their judgment, but their decisions will be based heavily on these quantitative factors. I think it is clear that this application of quantitative methods to policy decision has important similarities to the way that quantitative decision-making has been adopted in other areas of policy, including monetary policy.

** The Quantitative Impact of Changes in Fundamentals on Interest Rate Spreads ** Crisis prevention in emerging markets represents another area where quantitative modeling plays an integral role in policy formulation. In the last few years, the Treasury has developed a policy indicator model, called the “Blue Chip,” to help analyze developments in various emerging economies.  The model represents a management tool to systematically evaluate the risk of crisis. The Blue Chip scores serve as a rule to evaluate vulnerabilities by providing empirical signals of the level of relative risks, as well as shifts in those risks.  One policy response could be engagement with countries to encourage better policies and address potential vulnerabilities.

The Blue Chip index is constructed from five separate risk assessments from five different institutions.  The index considers much of the recent research on early warning systems, as well as the research on sovereign crises developed over the last twenty-years.  The Blue Chip index incorporates key variables such as the real exchange rate, domestic credit growth, M2/reserves, gross external financing requirements, global liquidity, as well as other measures.  We use percentile rankings to standardize scores across the five separate risk assessments and create a unified score for individual countries and emerging markets overall.  As with all quantitative indicators, discretion plays an essential role in the interpretation of the scores. The Blue Chip scores are evaluated by those Treasury staff who are most familiar with the countries and with various nuances influencing vulnerabilities.

Figure 1 shows the time-series pattern of this Blue Chip indicator during the past two years.  Note that the Blue Chip has tracked the broad movements in the emerging market spread fairly well. (The EMBI spread is shown in the figure in standard deviation units.) Granger-causality tests suggest that the Blue Chip has use as a leading indicator.

To the extent that the indicator is based on policy fundamentals, it gives an estimate of the amount by which the large recent decline in emerging market spreads is due fundamentals. Many analysts have been concerned in recent months that there has been overshooting of the spreads. Figure 1 does indicate some overshooting, but it is small relative to the decline in fundamentals as measure by the Blue Chip indicator.

** Changes in Emerging Market Contagion ** A third example relates to the problem of contagion in emerging markets, in which a financial crisis in one part of the world could spread to other parts of the world. Because contagion was such a large factor in the official sector policy responses to the financial crises in the 1990s, we have paid particular attention to it.  We have applied existing empirical work and have performed additional work when needed.

Our research reviews and work showed that much contagion was due to the interconnections between counties, and would not be automatic when countries are not substantially interconnected.  Moreover, if policy changes could be better anticipated and policies in different countries could be differentiated, the amount of contagion would decline. In fact we detected a decline in contagion starting in 2001 period, even before the crisis in Argentina.

For this reason we began to speak out about the decline in contagion in the spring and summer of 2001. We also tried to communicate better about our views on contagion and give the markets a clearer sense of our policy intentions, trying to minimize the surprise element as much as possible.

Figure 2 gives a simple graphic representation of the way in which contagion was so different following the Argentine default in 2001 compared with the Russian default in 1998.  As our empirical work began demonstrating in early 2001 and as our policy approach began reflecting this, there was a marked decline in contagion. The impact of Russian default in other parts of the world was far worse than the impact of Argentina in other parts of the world.

** Conclusion ** The success of quantitative modeling in the field of monetary policy is clear, and this is due in large part to the “translational economic” work by Dale Henderson, Richard Porter, and Peter Tinsley.  And, as the examples I have given here indicate, there is much potential in the further application of quantitative methods to policy.  I could only touch on a small number of issues we are working on at Treasury.  I would like to talk more about other applications, such as debt sustainability models or the development of a monetary framework for Iraq. It is clear that much remains to be accomplished in the area of applying economic models to practical policy problems.  I thank the organizers of this conference for promoting the discussion.

 

[[1]](https://home.treasury.gov/news/press-releases/js1322#_ftnref1_ftnref1)Bernanke, Ben S. (2004). “The Great Moderation,” at the meetings of the Eastern Economic Association, Washington, D.C., February 20.

[[2]](https://home.treasury.gov/news/press-releases/js1322#_ftnref2_ftnref2)Taylor, John B. (1998). “Applying Academic Research on Monetary Policy Rules: An Exercise in Translational Economics,” Harry G. Johnson lecture,*The Manchester School*, Vol. 66, Supplement 1, pp. 1-16.

 

**REPORTS**

- [Figure 1](https://home.treasury.gov/system/files/136/archive-documents/figure11.pdf)
- [Figure 2](https://home.treasury.gov/system/files/136/archive-documents/figure2.pdf)
